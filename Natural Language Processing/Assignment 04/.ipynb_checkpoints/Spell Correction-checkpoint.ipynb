{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Name: </b> MANAY, Justin Gabrielle A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise # 03: Spell Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I. Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_pattern = r'\\b[a-zA-Z]+\\-?\\'?[a-zA-Z]+\\b'\n",
    "\n",
    "# Load corpus\n",
    "text, tokens, vocab = [], [], []\n",
    "corpus = {\"Raw Text\": text, \"Tokens\": tokens, \"Vocabulary\": vocab}\n",
    "corpus_directory = \"data\\corpus\"\n",
    "files = os.listdir(corpus_directory)\n",
    "for file_name in files:\n",
    "    with open(os.path.join(corpus_directory, file_name), 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            text.append(line)\n",
    "            tokens.extend(re.findall(re_pattern, line))\n",
    "            tokens.append(\"START/END\")\n",
    "            vocab.extend(list(set(re.findall(re_pattern, line))))\n",
    "\n",
    "vocab.append(\"START/END\")\n",
    "            \n",
    "# Find vocabulary set\n",
    "total_vocabulary = set()\n",
    "total_vocabulary |= set(corpus[\"Vocabulary\"])\n",
    "\n",
    "total_vocabulary = list(total_vocabulary)\n",
    "vocabulary_count = len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and tokenize the corpus. We used the 2016 News Typical corpus from the Leipzig Corpora Collection, which can be accessed through the following [link](http://wortschatz.uni-leipzig.de/en/download/). The corpus contains up to 1 million sentences (We used only 300,000 sentences so as to keep the program reasonably fast) \"randomly selected from newspaper texts or texts randomly collected from the web.\"\n",
    "\n",
    "Since we used the News Typical corpus, the words used in the sentences are simpler and are thus more frequently used in English. However, since the sentences are taken from newspaper/web texts, they will contain proper nouns and acronyms, which are not meant to be spell-checked. Also, the corpus will also miss out on a few frequently occurring English words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences are loaded into the dictionary `corpus`, as well as the tokens and the vocabulary. At the end of every sentence, we append the token `START/END` to signify the beginning of a new sentence/end of an old one. This is in the event that we employ a bigram model to prevent the tokens of one sentence from seeping into another. We then generate the vocabulary from the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency list for unigram language model\n",
    "freqDict = dict((word, 0) for word in corpus[\"Vocabulary\"])\n",
    "for word in corpus[\"Tokens\"]:\n",
    "    if word == \"START/END\":\n",
    "        continue\n",
    "    else:\n",
    "        freqDict[word] += 1 / vocabulary_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be using the unigram language model for this assignment, which entails computing the frequency of each token in the vocabulary, as is done by the code above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II. Error Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load confusion matrix data\n",
    "confusion_directory = \"data\\confusion\"        \n",
    "file_name = os.listdir(confusion_directory)[0]\n",
    "confusion = {\"Ins\": {}, \"Del\": {}, \"Sub\": {}, \"Trans\": {}}\n",
    "with open(os.path.join(confusion_directory, file_name), 'r') as f:\n",
    "    for line in f:\n",
    "        delim1 = line.find(\"|\")\n",
    "        delim2 = line.find(\"\\t\")\n",
    "        wrong = line[: delim1]\n",
    "        right = line[delim1 + 1 : delim2]\n",
    "        \n",
    "        # Use a regex pattern for the frequency\n",
    "        freq_re_pattern = r'\\b[0-9]+\\b'\n",
    "        freq = int(re.findall(freq_re_pattern, line)[0])\n",
    "        \n",
    "        # Classify errors\n",
    "        if len(wrong) == len(right) and len(wrong) == 2:\n",
    "            confusion[\"Trans\"][(wrong, right)] = freq\n",
    "        elif len(wrong) == len(right):\n",
    "            confusion[\"Sub\"][(wrong, right)] = freq\n",
    "        elif len(wrong) > len(right):\n",
    "            confusion[\"Ins\"][(wrong, right)] = freq\n",
    "        elif len(wrong) < len(right):\n",
    "            confusion[\"Del\"][(wrong, right)] = freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the error model, we use data on the frequency of certain one-distance edits in the English language. This [data](https://norvig.com/ngrams/) was collected by Peter Norvig from Wikipedia and Roger Mutton and details how frequently one makes one-distance edits or misspellings based on other corpora. The misspellings are given in the form `wrong|right`, where `wrong` represents the misspelling and `right` represents the correct spelling. For example, `r|re` has a frequency of 392, signifying that based on the corpus, a word that should've been spelled with an re was spelled with an r 392 times.\n",
    "\n",
    "We extract `wrong`, `right` and the frequency from the text file `count_1edit.txt` which was downloaded from the above link. Due to some strange formatting, we opted to extract `wrong` and `right` without using regex by using \"|\" and \"\\t\" as delimiters. We extracted the frequency via regex. \n",
    "\n",
    "We then classified the errors based on whether they are transposition, substitution, insertion or deletion errors, after which we load them into the dictionary `confusion`. `confusion` is a dictionary within a dictionary, containing all the one-distance edits sorted by type of error. We use the tuple `(wrong, right)` as our keys and the frequency as our values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "# insertion confusion matrix\n",
    "ins_wrong_vals = list(set([key[0] for key in confusion[\"Ins\"].keys()]))\n",
    "ins_right_vals = list(set([key[1] for key in confusion[\"Ins\"].keys()]))\n",
    "insMatrix = np.zeros((len(ins_wrong_vals), len(ins_right_vals)))\n",
    "\n",
    "for i in range(len(ins_wrong_vals)):\n",
    "    for j in range(len(ins_right_vals)):       \n",
    "        try:\n",
    "            insMatrix[i, j] = confusion[\"Ins\"][(ins_wrong_vals[i], ins_right_vals[j])]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Perform add-one smoothing            \n",
    "for i in range(len(ins_wrong_vals)):\n",
    "    insMatrix[i, ] = (insMatrix[i, ] + 1) / (np.sum(insMatrix[i,]) + len(ins_wrong_vals) * len(ins_right_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to creating the confusion matrices for all possible error types. The rows in the confusion matrix represent the wrong values while the columns represent the right values. We first make a list of all unique wrong and right values and use them to create the confusion matrix `insMatrix` based on the values in `confusion`, employing exception handling in case the pair of values is not a key in the dictionary. \n",
    "\n",
    "Since we will ultimately be multiplying the error probability and the unigram probability and the confusion matrix has a lot of zeroes, there is a chance that we would end up with a probability of zero. Thus, we employ add-one smoothing to remove all the zeroes, adding one to all values and then dividing by the number of cells in the matrix.\n",
    "\n",
    "We then repeat this procedure for the other three matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitution confusion matrix\n",
    "sub_wrong_vals = list(set([key[0] for key in confusion[\"Sub\"].keys()]))\n",
    "sub_right_vals = list(set([key[1] for key in confusion[\"Sub\"].keys()]))\n",
    "subMatrix = np.zeros((len(sub_wrong_vals), len(sub_right_vals)))\n",
    "\n",
    "for i in range(len(sub_wrong_vals)):\n",
    "    for j in range(len(sub_right_vals)):       \n",
    "        try:\n",
    "            subMatrix[i, j] = confusion[\"Sub\"][(sub_wrong_vals[i], sub_right_vals[j])]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "for i in range(len(sub_wrong_vals)):\n",
    "    subMatrix[i, ] = (subMatrix[i, ] + 1) / (np.sum(subMatrix[i,]) + len(sub_wrong_vals) * len(sub_right_vals))\n",
    "    \n",
    "# deletion confusion matrix\n",
    "del_wrong_vals = list(set([key[0] for key in confusion[\"Del\"].keys()]))\n",
    "del_right_vals = list(set([key[1] for key in confusion[\"Del\"].keys()]))\n",
    "delMatrix = np.zeros((len(del_wrong_vals), len(del_right_vals)))\n",
    "\n",
    "for i in range(len(del_wrong_vals)):\n",
    "    for j in range(len(del_right_vals)):       \n",
    "        try:\n",
    "            delMatrix[i, j] = confusion[\"Del\"][(del_wrong_vals[i], del_right_vals[j])]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "for i in range(len(del_wrong_vals)):\n",
    "    delMatrix[i, ] = (delMatrix[i, ] + 1) / (np.sum(delMatrix[i,]) + len(del_wrong_vals) * len(del_right_vals))\n",
    "\n",
    "# transposition confusion matrix\n",
    "trans_wrong_vals = list(set([key[0] for key in confusion[\"Trans\"].keys()]))\n",
    "trans_right_vals = list(set([key[1] for key in confusion[\"Trans\"].keys()]))\n",
    "transMatrix = np.zeros((len(trans_wrong_vals), len(trans_right_vals)))\n",
    "\n",
    "for i in range(len(trans_wrong_vals)):\n",
    "    for j in range(len(trans_right_vals)):       \n",
    "        try:\n",
    "            transMatrix[i, j] = confusion[\"Trans\"][(trans_wrong_vals[i], trans_right_vals[j])]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "for i in range(len(trans_wrong_vals)):\n",
    "    transMatrix[i, ] = (transMatrix[i, ] + 1) / (np.sum(transMatrix[i,]) + len(trans_wrong_vals) * len(trans_right_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III. Selecting Candidates through Edit Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our language and error models set up, we can now create a procedure to generate all the possible candidates in the spell correction procedure. To do this, we use the Damerau-Levenshtein minimum edit distance algorithm and choose all words in `total_vocabulary` which are within one edit distance from the word to be spell-corrected. The implementation of this algorithm is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for Damerau-Levenshtein edit distance\n",
    "def minEditDistance(word1, word2):\n",
    "    # Initialize matrices\n",
    "    distMatrix = np.zeros((len(word1) + 1, len(word2) + 1))\n",
    "    ptrMatrix = [[[] for j in range(len(word2) + 1)] for i in range(len(word1) + 1)]\n",
    "    insCost = 1\n",
    "    delCost = 1\n",
    "    transCost = 1\n",
    "     \n",
    "    distMatrix[0, 0] = 0     \n",
    "    for i in range(1, len(word1) + 1):\n",
    "        distMatrix[i, 0] = i * delCost\n",
    "        ptrMatrix[i][0].append(\"UP\")\n",
    "    for j in range(1, len(word2) + 1):\n",
    "        distMatrix[0, j] = j * insCost\n",
    "        ptrMatrix[0][j].append(\"LEFT\")\n",
    "                \n",
    "    # Fill up matrices\n",
    "    for i in range(1, len(word1) + 1):\n",
    "        for j in range(1, len(word2) + 1):\n",
    "            \n",
    "            if word1[i - 1] != word2[j - 1]:\n",
    "                subCost = 1\n",
    "            else:\n",
    "                subCost = 0\n",
    "            \n",
    "            distMatrix[i, j] = min(distMatrix[i - 1, j] + insCost, distMatrix[i, j - 1] + delCost, distMatrix[i - 1, j - 1] + subCost)\n",
    "            if word1[i - 1] == word2[j - 2] and word1[i - 2] == word2[j - 1] and i > 1 and j > 1:\n",
    "                distMatrix[i, j] = min(distMatrix[i, j], distMatrix[i - 2, j - 2] + transCost)\n",
    "            \n",
    "            if distMatrix[i, j] == distMatrix[i - 1, j] + insCost:\n",
    "                ptrMatrix[i][j].append(\"UP\")\n",
    "            if distMatrix[i, j] == distMatrix[i, j - 1] + delCost:\n",
    "                ptrMatrix[i][j].append(\"LEFT\")\n",
    "            if distMatrix[i, j] == distMatrix[i - 1, j - 1] + subCost:\n",
    "                ptrMatrix[i][j].append(\"DIAG\")\n",
    "            if distMatrix[i, j] == distMatrix[i - 2, j - 2] + transCost:\n",
    "                ptrMatrix[i][j].append(\"FLIP\")\n",
    "\n",
    "    # Return min edit distance\n",
    "    dist = int(distMatrix[len(word1), len(word2)])\n",
    "    \n",
    "    # Form backtrace path using ptrMatrix\n",
    "    backtracePath = []\n",
    "    rownum, colnum = len(word1), len(word2)\n",
    "    \n",
    "    # Prioritize diagonal movement. Upward and leftward movement are interchangeable.\n",
    "    while (rownum != 0) or (colnum != 0):\n",
    "        backtracePath.append((rownum, colnum))\n",
    "        if \"DIAG\" in ptrMatrix[rownum][colnum]:\n",
    "            rownum -= 1\n",
    "            colnum -= 1\n",
    "        elif \"LEFT\" in ptrMatrix[rownum][colnum]:\n",
    "            colnum -= 1\n",
    "        elif \"UP\" in ptrMatrix[rownum][colnum]:\n",
    "            rownum -= 1\n",
    "        elif \"FLIP\" in ptrMatrix[rownum][colnum]:\n",
    "            rownum -= 2\n",
    "            colnum -= 2\n",
    "   \n",
    "    # Reverse backtracePath\n",
    "    backtracePath = backtracePath[::-1]\n",
    "        \n",
    "    # Print alignment.\n",
    "    word1Print = \"\"\n",
    "    alignPrint = \"\"\n",
    "    word2Print = \"\"\n",
    "    \n",
    "    for rownum, colnum in backtracePath:\n",
    "        if \"DIAG\" in ptrMatrix[rownum][colnum]:\n",
    "            word1Print += word1[rownum - 1]\n",
    "            word2Print += word2[colnum - 1]\n",
    "            if word1[rownum - 1] == word2[colnum - 1]:\n",
    "                alignPrint += \"M\"\n",
    "            else:\n",
    "                alignPrint += \"S\"\n",
    "        elif \"FLIP\" in ptrMatrix[rownum][colnum]:\n",
    "            word1Print += word1[rownum - 2]\n",
    "            word1Print += word1[rownum - 1]\n",
    "            alignPrint += \"TT\"\n",
    "            word2Print += word2[colnum - 2]\n",
    "            word2Print += word2[colnum - 1]\n",
    "        elif \"LEFT\" in ptrMatrix[rownum][colnum]:\n",
    "            word1Print += \" \"\n",
    "            alignPrint += \"I\"\n",
    "            word2Print += word2[colnum - 1]\n",
    "        elif \"UP\" in ptrMatrix[rownum][colnum]:\n",
    "            word1Print += word1[rownum - 1]\n",
    "            alignPrint += \"D\"\n",
    "            word2Print += \" \"\n",
    "    return {\"Distance\": dist, \"Misspelled\": word2Print, \"Correct\": word1Print, \"Alignment\": alignPrint}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our own edit distance function since we want to also align the correct and the misspelled words. The function is roughly the same as the Levenshtein edit distance function we implemented earlier but it now takes transpositions into account. \n",
    "\n",
    "The costs for substitution, insertion, deletion and transposition are kept similar so that the algorithm isn't biased towards one type of editing. Also, the algorithm prioritizes matches/substitutions, and then tranpositions, and then insertions and then deletions.\n",
    "\n",
    "The output of the function is a dictionary with the edit distance, the aligned correct and misspelled words and the alignment (i.e., the series of operations needed to go from the correct word to the misspelled word.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART IV. Combining the Error and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now ask the user for input and combine everything discussed in Parts I through III to form the spellchecker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCorrect():\n",
    "    # Get (and tokenize, if using a bigram/trigram model) an input value/hard coded value\n",
    "    inputWord = input(\"Please enter a string: \")\n",
    "\n",
    "    # Screen candidates from vocabulary list\n",
    "    candidateList = []\n",
    "\n",
    "    if inputWord in total_vocabulary:\n",
    "        print(\"No error\")\n",
    "    else:\n",
    "        for word in total_vocabulary:\n",
    "            dist = minEditDistance(word, inputWord)\n",
    "            if dist[\"Distance\"] == 1:\n",
    "                candidateList.append(word)\n",
    "        \n",
    "        if candidateList == []:\n",
    "            print(\"No word in corpus close enough to input\")\n",
    "        \n",
    "        probDict = {}\n",
    "        for candidate in candidateList:\n",
    "            dist = minEditDistance(candidate, inputWord)\n",
    "\n",
    "            unigram_prob = freqDict[candidate]\n",
    "            if \"I\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"I\")\n",
    "\n",
    "                if misspell_idx != len(inputWord) - 1:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx: misspell_idx + 2]\n",
    "                else:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx - 2: misspell_idx]\n",
    "                    \n",
    "                matrix_wrong_idx = ins_wrong_vals.index(wrong)\n",
    "\n",
    "                if misspell_idx != 0:\n",
    "                    right = dist[\"Correct\"][misspell_idx - 1]\n",
    "                else:\n",
    "                    right = dist[\"Correct\"][misspell_idx + 1]\n",
    "                matrix_right_idx = ins_right_vals.index(right)  \n",
    "\n",
    "                error_prob = insMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"S\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"S\")\n",
    "\n",
    "                wrong = dist[\"Misspelled\"][misspell_idx]\n",
    "                matrix_wrong_idx = sub_wrong_vals.index(wrong)\n",
    "\n",
    "                right = dist[\"Correct\"][misspell_idx]\n",
    "                matrix_right_idx = sub_right_vals.index(right)\n",
    "\n",
    "                error_prob = subMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"D\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"D\")\n",
    "\n",
    "                if misspell_idx != 0:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx - 1]\n",
    "                else:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx + 1]\n",
    "                matrix_wrong_idx = del_wrong_vals.index(wrong)\n",
    "\n",
    "                if misspell_idx != len(candidate) - 1:    \n",
    "                    right = dist[\"Correct\"][misspell_idx - 1: misspell_idx + 1]\n",
    "                else:\n",
    "                    right = dist[\"Correct\"][misspell_idx - 2: misspell_idx]\n",
    "                matrix_right_idx = del_right_vals.index(right)\n",
    "\n",
    "                error_prob = delMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"TT\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"T\")\n",
    "\n",
    "                wrong = dist[\"Misspelled\"][misspell_idx: misspell_idx + 2]\n",
    "                matrix_wrong_idx = trans_wrong_vals.index(wrong)\n",
    "\n",
    "                right = dist[\"Correct\"][misspell_idx: misspell_idx + 2]\n",
    "                matrix_right_idx = trans_right_vals.index(right)\n",
    "\n",
    "                error_prob = transMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "\n",
    "            prob = unigram_prob * error_prob\n",
    "            probDict[candidate] = prob\n",
    "\n",
    "        for key in list(probDict.keys()):\n",
    "            sum_prob = np.sum(list(probDict.values()))\n",
    "            probDict[key] = probDict[key] / sum_prob\n",
    "\n",
    "        # Sort dictionary by values\n",
    "        # from: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "        sorted_probDict = sorted(probDict.items(), key = lambda kv: kv[1], reverse = True)\n",
    "\n",
    "        for candidate, prob in sorted_probDict:\n",
    "            print(candidate + \" (\" + str(prob) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After asking for input, we first check if the word is in `total_vocabulary` (i.e., we check if the word has been correctly spelled). If not, we proceed with the spell correction.\n",
    "\n",
    "First, we generate all possible candidates in the spell correction procedure by checking which words in `total_vocabulary` are within one edit distance from `inputWord`. We then get the unigram probability `unigram_prob` from the frequency dictionary we generated in Part I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error probability `error_prob` is more difficult to compute. We first use the alignment between the correct word and the misspelled word to check which operation (insertion, substitution, deletion or transposition) had been performed. After this, we check where this operation occurs and get the letters concerned from `dist[\"Correct\"]` and `dist[\"Misspelled\"]`.\n",
    "\n",
    "This is easy in the case of substitutions and transpositions. This easily becomes difficult in the case of either an insertion or a deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actress\n",
      "MMDMMMM\n",
      "ac ress\n"
     ]
    }
   ],
   "source": [
    "example1 = minEditDistance(\"actress\", \"acress\")\n",
    "print(example1[\"Correct\"])\n",
    "print(example1[\"Alignment\"])\n",
    "print(example1[\"Misspelled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily consider the pairing ` ` and `t`, but checking the deletion confusion matrix,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \" in del_right_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` ` is not part of `del_right_vals`! We can consider the pairing `c` (one letter to the left of the deletion) and `ct` (letter at the deletion and one letter to its left) or the pairing `t` and `tr `and sure enough,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ct\" in del_right_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"tr\" in del_right_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also run into a problem if the error happens to be in the beginning/end of a word. Consider this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " up\n",
      "IMM\n",
      "tup\n"
     ]
    }
   ],
   "source": [
    "example2 = minEditDistance(\"up\", \"tup\")\n",
    "print(example2[\"Correct\"])\n",
    "print(example2[\"Alignment\"])\n",
    "print(example2[\"Misspelled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the rule from awhile ago and choose one letter to the left of the insertion, since it occurs at the beginning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Norvig's confusion matrix data, a lot of the misspellings involving insertions and deletions are given in the form of two letters. However, the choice of which letters to consider for the spell correction is arbitrary. Thus, we use the rule from earlier (prioritize letters to the left of the insertion/deletion) and modify it accordingly whenever the error is at the beginning/end of the word. For example, if the error is at the beginning, we simply move to the right of the error and if it is at the end, we move to the left of the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining which letters to consider, we look them up in the list of right and wrong values to determine where in the confusion matrix to look. We get `error_prob` afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the nosiy channel model, the correctly spelled word is the one which maximizes the product `unigram_prob * error_prob`. Thus, we compute this product for all candidates and store it in `probDict`. We then normalize the probabilities by dividing them by the sum of the probabilities, to make for easier comparability. We then sort the dictionary by values (in descending order) and then print all candidates, with the most likely correction up front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the program works, we use an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: wrip\n",
      "wrap (0.006592001262366998)\n",
      "trip (0.002536683010987814)\n",
      "rip (0.00046466501117905774)\n",
      "writ (0.0003337265006042013)\n",
      "grip (4.203277283906145e-05)\n",
      "whip (1.866035138671428e-05)\n",
      "drip (1.1633235451019149e-05)\n"
     ]
    }
   ],
   "source": [
    "spellCorrect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word `wrip`, the program suggests the corrections `wrap`, `trip`, `grip`, `whip`, `writ`, `drip` and `rip`. However, based on the noisy channel model, the most likely spelling correction is the word `wrap`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART IV. Limitations of the Program "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program has a number of limitations due to the choice of corpus and confusion matrix data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one, because we use the corpus as our source of words, there is the possibility of encountering proper nouns, acronyms and abbreviations, as shown in the example below. The word `twp` ,for example, is an abbreviation/acronym and should not be counted as a correction for the word `trp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: trp\n",
      "try (0.0912031670838464)\n",
      "trip (0.00041494453398646707)\n",
      "tip (5.606767653398902e-05)\n",
      "top (3.532722008583494e-05)\n",
      "trap (2.8105938048280677e-05)\n",
      "tap (2.1950807310506906e-05)\n",
      "tarp (2.138440727597923e-06)\n",
      "twp (1.5307220066271836e-06)\n",
      "tpp (1.2245471201697923e-06)\n",
      "irp (7.876104834396554e-07)\n",
      "ttp (7.112139996374704e-07)\n",
      "trt (5.970346583874939e-07)\n",
      "trc (4.859423781402116e-07)\n",
      "tcp (3.571630238224614e-07)\n",
      "thp (2.5406090963834277e-07)\n",
      "tro (2.160949262322328e-07)\n",
      "tri (1.0794661190039474e-07)\n",
      "tre (5.427487479304777e-08)\n",
      "tru (5.403922990354686e-08)\n",
      "zrp (4.92251657194827e-08)\n",
      "rp (1.963707131901836e-08)\n",
      "tp (1.9567735778192652e-08)\n",
      "tpr (1.500209490112472e-08)\n"
     ]
    }
   ],
   "source": [
    "spellCorrect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also encounter errors involving double letters. In cases where there should have been double letters (deletion error), the spell corrector works just fine. However, when we have mistakenly doubled a letter (insertion error), the program throws an error. This is because the confusion matrix data does not account for double letters like `ll` or `rr` (as in `parrent`, shown below). We can modify the program accordingly to account for double letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: parrent\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'rr' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e40b4d7cd63a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspellCorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-925cc38b657a>\u001b[0m in \u001b[0;36mspellCorrect\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mwrong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Misspelled\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmisspell_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmisspell_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mmatrix_wrong_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_wrong_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmisspell_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'rr' is not in list"
     ]
    }
   ],
   "source": [
    "spellCorrect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing the program,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCorrectEdited():\n",
    "    # Get (and tokenize, if using a bigram/trigram model) an input value/hard coded value\n",
    "    inputWord = input(\"Please enter a string: \")\n",
    "\n",
    "    # Screen candidates from vocabulary list\n",
    "    candidateList = []\n",
    "\n",
    "    if inputWord in total_vocabulary:\n",
    "        print(\"No error\")\n",
    "    else:\n",
    "        for word in total_vocabulary:\n",
    "            dist = minEditDistance(word, inputWord)\n",
    "            if dist[\"Distance\"] == 1:\n",
    "                candidateList.append(word)\n",
    "        \n",
    "        if candidateList == []:\n",
    "            print(\"No word in corpus close enough to input\")\n",
    "\n",
    "        probDict = {}\n",
    "        for candidate in candidateList:\n",
    "            dist = minEditDistance(candidate, inputWord)\n",
    "\n",
    "            unigram_prob = freqDict[candidate]\n",
    "            if \"I\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"I\")\n",
    "\n",
    "                if misspell_idx != len(inputWord) - 1:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx: misspell_idx + 2]\n",
    "                    # Account for mistaken double letters\n",
    "                    if wrong[0] == wrong[1]:\n",
    "                        wrong = dist[\"Misspelled\"][misspell_idx - 1: misspell_idx + 1]\n",
    "                else:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx - 2: misspell_idx]\n",
    "                matrix_wrong_idx = ins_wrong_vals.index(wrong)\n",
    "\n",
    "                if misspell_idx != 0:\n",
    "                    right = dist[\"Correct\"][misspell_idx - 1]\n",
    "                else:\n",
    "                    right = dist[\"Correct\"][misspell_idx + 1]\n",
    "                matrix_right_idx = ins_right_vals.index(right)  \n",
    "\n",
    "                error_prob = insMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"S\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"S\")\n",
    "\n",
    "                wrong = dist[\"Misspelled\"][misspell_idx]\n",
    "                matrix_wrong_idx = sub_wrong_vals.index(wrong)\n",
    "\n",
    "                right = dist[\"Correct\"][misspell_idx]\n",
    "                matrix_right_idx = sub_right_vals.index(right)\n",
    "\n",
    "                error_prob = subMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"D\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"D\")\n",
    "\n",
    "                if misspell_idx != 0:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx - 1]\n",
    "                else:\n",
    "                    wrong = dist[\"Misspelled\"][misspell_idx + 1]\n",
    "                matrix_wrong_idx = del_wrong_vals.index(wrong)\n",
    "\n",
    "                if misspell_idx != len(candidate) - 1:    \n",
    "                    right = dist[\"Correct\"][misspell_idx - 1: misspell_idx + 1]\n",
    "                else:\n",
    "                    right = dist[\"Correct\"][misspell_idx - 2: misspell_idx]\n",
    "                matrix_right_idx = del_right_vals.index(right)\n",
    "\n",
    "                error_prob = delMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "            elif \"TT\" in dist[\"Alignment\"]:\n",
    "                misspell_idx = dist[\"Alignment\"].index(\"T\")\n",
    "\n",
    "                wrong = dist[\"Misspelled\"][misspell_idx: misspell_idx + 2]\n",
    "                matrix_wrong_idx = trans_wrong_vals.index(wrong)\n",
    "\n",
    "                right = dist[\"Correct\"][misspell_idx: misspell_idx + 2]\n",
    "                matrix_right_idx = trans_right_vals.index(right)\n",
    "\n",
    "                error_prob = transMatrix[matrix_wrong_idx, matrix_right_idx]\n",
    "\n",
    "            prob = unigram_prob * error_prob\n",
    "            probDict[candidate] = prob\n",
    "\n",
    "        for key in list(probDict.keys()):\n",
    "            sum_prob = np.sum(list(probDict.values()))\n",
    "            probDict[key] = probDict[key] / sum_prob\n",
    "\n",
    "        # Sort dictionary by values\n",
    "        # from: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "        sorted_probDict = sorted(probDict.items(), key = lambda kv: kv[1], reverse = True)\n",
    "\n",
    "        for candidate, prob in sorted_probDict:\n",
    "            print(candidate + \" (\" + str(prob) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: parrent\n",
      "parent (0.04757196007312527)\n",
      "warrent (0.00031485278154048257)\n"
     ]
    }
   ],
   "source": [
    "spellCorrectEdited()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since we choose the letter before and on the insertion, the program still throws an error when the double letter error occurs at the beginning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: pparent\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-39acb182827a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspellCorrectEdited\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-69528755e847>\u001b[0m in \u001b[0;36mspellCorrectEdited\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mwrong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Misspelled\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmisspell_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmisspell_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mmatrix_wrong_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_wrong_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmisspell_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: '' is not in list"
     ]
    }
   ],
   "source": [
    "spellCorrectEdited()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just some limitations associated with our choice of corpus and confusion matrix data. Ideally, one should create his/her own corpus and confusion matrix data and suit it to his/her own needs.\n",
    "\n",
    "For a simple spell checker, for example, a list of sentences with high-frequency English words would be ideal. As for the confusion matrix data, limiting `wrong_vals` and `right_vals` to single characters would make the program much simpler and would make dealing with cases like the one above (double letters) easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Goldhahn, D., Eckart, T., & Quasthoff, U. (2012, May). Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages. In <i> LREC </i> (Vol. 29, pp. 31-43).\n",
    "\n",
    "[2] P. Norvig. (2009). Natural language corpus data. <i> Beautiful data </i>, 219-242."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
